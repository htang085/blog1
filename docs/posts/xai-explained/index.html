<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.5.57">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Hui Tang">
<meta name="dcterms.date" content="2025-01-18">

<title>The Evolution of Explainable AI (XAI) in Modern Machine Learning – Blog</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
</style>


<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<script src="../../site_libs/quarto-html/quarto.js"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>


<link rel="stylesheet" href="../../styles.css">
</head>

<body class="nav-fixed fullcontent">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top quarto-banner">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a class="navbar-brand" href="../../index.html">
    <span class="navbar-title">Blog</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" role="menu" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../about.html"> 
<span class="menu-text">About</span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/"> <i class="bi bi-github" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://twitter.com"> <i class="bi bi-twitter" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
            <div class="quarto-navbar-tools">
</div>
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<header id="title-block-header" class="quarto-title-block default page-columns page-full">
  <div class="quarto-title-banner page-columns page-full">
    <div class="quarto-title column-body">
      <h1 class="title">The Evolution of Explainable AI (XAI) in Modern Machine Learning</h1>
                                <div class="quarto-categories">
                <div class="quarto-category">Machine Learning</div>
                <div class="quarto-category">Data Science</div>
                <div class="quarto-category">Explainable AI</div>
              </div>
                  </div>
  </div>
    
  
  <div class="quarto-title-meta">

      <div>
      <div class="quarto-title-meta-heading">Author</div>
      <div class="quarto-title-meta-contents">
               <p>Hui Tang </p>
            </div>
    </div>
      
      <div>
      <div class="quarto-title-meta-heading">Published</div>
      <div class="quarto-title-meta-contents">
        <p class="date">January 18, 2025</p>
      </div>
    </div>
    
      
    </div>
    
  
  </header><div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    
<!-- main -->
<main class="content quarto-banner-title-block" id="quarto-document-content">





<section id="introduction" class="level1">
<h1>Introduction</h1>
<p>Artificial Intelligence (AI) has revolutionized industries, enabling advancements in healthcare, finance, transportation, and beyond. However, despite its transformative potential, many AI systems operate as <strong>“black boxes”</strong>, meaning their internal decision-making processes remain opaque. This lack of transparency raises pressing concerns about <strong>fairness</strong>, <strong>accountability</strong>, and <strong>trust</strong> in AI.</p>
<p><strong>Explainable AI (XAI)</strong> addresses these challenges by making AI systems more interpretable and their decisions more understandable. In this blog, we’ll delve into the <strong>evolution of XAI</strong>, explore its <strong>key concepts and methods</strong>, analyze <strong>real-world applications</strong>, and consider the <strong>challenges</strong> it faces as it moves toward widespread adoption.</p>
<hr>
</section>
<section id="what-is-explainable-ai-xai" class="level1">
<h1>What is Explainable AI (XAI)?</h1>
<p>Explainable AI (XAI) refers to a set of techniques, tools, and methods designed to make the decision-making processes of AI models transparent and comprehensible. Unlike traditional machine learning models, which often prioritize predictive accuracy over interpretability, XAI strikes a balance between the two.</p>
<section id="key-concepts-in-xai" class="level2">
<h2 class="anchored" data-anchor-id="key-concepts-in-xai">Key Concepts in XAI</h2>
<section id="transparency" class="level3">
<h3 class="anchored" data-anchor-id="transparency">Transparency</h3>
<p>Transparency ensures that the decision-making process of an AI system is accessible and visible. For instance, rule-based systems like decision trees are inherently transparent, whereas neural networks often require post-hoc interpretability techniques to make sense of their complex layers.</p>
</section>
<section id="interpretability" class="level3">
<h3 class="anchored" data-anchor-id="interpretability">Interpretability</h3>
<p>Interpretability measures how easily a human can understand a model’s predictions or internal mechanics. For example, a simple linear regression model is more interpretable than a deep learning model because its relationships are mathematically explicit.</p>
</section>
<section id="trustworthiness" class="level3">
<h3 class="anchored" data-anchor-id="trustworthiness">Trustworthiness</h3>
<p>Trust in AI systems is built when users feel confident in their decisions. Trustworthiness is closely linked to transparency and interpretability. For example, a hospital using AI for cancer diagnosis needs to ensure its system explains why a certain tumor is classified as malignant.</p>
<p>These principles form the foundation of XAI, bridging the gap between sophisticated algorithms and human understanding.</p>
<hr>
</section>
</section>
</section>
<section id="why-does-xai-matter" class="level1">
<h1>Why Does XAI Matter?</h1>
<p>AI is increasingly deployed in <strong>high-stakes domains</strong> where decisions significantly impact people’s lives. Examples include:</p>
<ol type="1">
<li><p><strong>Healthcare</strong>: AI is used for diagnosing diseases, recommending treatments, and analyzing patient data. Without explainability, physicians may struggle to trust or validate AI-driven recommendations.</p></li>
<li><p><strong>Finance</strong>: Models decide on loan approvals, fraud detection, and risk assessments. A lack of transparency could lead to unfair credit denials or biased financial practices.</p></li>
<li><p><strong>Criminal Justice</strong>: Risk assessment algorithms help inform sentencing or parole decisions. If these systems are opaque, they may perpetuate biases, undermining public trust in legal systems.</p></li>
</ol>
<section id="risks-of-opaque-ai" class="level3">
<h3 class="anchored" data-anchor-id="risks-of-opaque-ai">Risks of Opaque AI</h3>
<p>The absence of explainability introduces several risks:<br>
- <strong>Ethical Issues</strong>: Hidden biases in data can lead to unfair decisions.<br>
- <strong>Regulatory Violations</strong>: Laws like the EU’s GDPR require explainability for AI decisions affecting individuals.<br>
- <strong>Loss of Trust</strong>: Users are less likely to adopt AI systems they don’t understand or trust.</p>
<p>Consider a real-world example: In 2020, it was revealed that a healthcare algorithm in the U.S. systematically underestimated the healthcare needs of Black patients. This bias was rooted in historical data and could have been mitigated with better explainability tools.</p>
<hr>
</section>
</section>
<section id="tools-and-methods-in-xai" class="level1">
<h1>Tools and Methods in XAI</h1>
<p>Several tools and techniques are available to enhance the interpretability of AI systems. These can be broadly categorized into <strong>model-agnostic</strong> methods (applicable to any model) and <strong>model-specific</strong> methods (tailored to specific algorithms).</p>
<section id="shap-shapley-additive-explanations" class="level2">
<h2 class="anchored" data-anchor-id="shap-shapley-additive-explanations">1. SHAP (SHapley Additive Explanations)</h2>
<p>SHAP uses concepts from cooperative game theory to assign importance values to features based on their contribution to the model’s predictions. SHAP provides both <strong>global explanations</strong> (understanding the overall model behavior) and <strong>local explanations</strong> (analyzing individual predictions).</p>
<ul>
<li><strong>Example</strong>: For a loan approval model, a SHAP plot might reveal that “income” positively contributes to approval, while “credit history” negatively affects it.</li>
</ul>
<div style="text-align: center;">
<img src="california_beeswarm.png" alt="SHAP Summary Plot" width="600">
<p style="text-align: center; font-size: 14px; color: #555; margin-top: 10px;">
Figure 1: SHAP summary plot showing feature importance and their contributions to model predictions.
</p>
</div>
</section>
<section id="lime-local-interpretable-model-agnostic-explanations" class="level2">
<h2 class="anchored" data-anchor-id="lime-local-interpretable-model-agnostic-explanations">2. LIME (Local Interpretable Model-Agnostic Explanations)</h2>
<p>LIME approximates a model locally by building an interpretable surrogate model (e.g., linear regression) around a specific prediction. This method is particularly useful for black-box models like neural networks.</p>
<ul>
<li><strong>Example</strong>: LIME explains why an AI system classifies an email as spam by showing the words most likely to trigger the classification.</li>
</ul>
<div style="display: flex; flex-direction: column; align-items: flex-start; margin-left: 50px;">
<img src="lime_explanation_plot.png" alt="LIME Explanation Plot" width="600">
<p style="text-align: center; font-size: 14px; color: #555; margin-top: 10px;">
Figure 2: LIME explanation plot highlighting feature contributions to an individual prediction.
</p>
</div>
</section>
<section id="feature-importance-visualizations" class="level2">
<h2 class="anchored" data-anchor-id="feature-importance-visualizations">3. Feature Importance Visualizations</h2>
<p>Many machine learning libraries, such as <code>scikit-learn</code> and <code>XGBoost</code>, include built-in tools to visualize feature importance. These tools help identify the most impactful features in a dataset. In the case of Random Forest, feature importance is derived from the average decrease in impurity across all the decision trees in the forest. This makes it a reliable measure for understanding how different features contribute to the model’s predictions.</p>
<div style="text-align: center;">
<img src="random_forest_feature_importance_reversed.png" alt="Random Forest Feature Importance" width="700">
<p style="text-align: center; font-size: 14px; color: #555; margin-top: 10px;">
Figure 3: Feature importance plot from a Random Forest Classifier. Each bar represents the average decrease in impurity associated with the corresponding feature.
</p>
</div>
</section>
<section id="counterfactual-explanations" class="level2">
<h2 class="anchored" data-anchor-id="counterfactual-explanations">4. Counterfactual Explanations</h2>
<p>Counterfactuals answer “what if” questions, such as: “What changes would result in a different outcome?” These explanations are actionable and help users understand how to achieve a desired result.</p>
<ul>
<li><strong>Example</strong>: A counterfactual explanation for a rejected loan might suggest increasing income by $5,000 or reducing outstanding debt by 20%.</li>
</ul>
<div style="text-align: center;">
<img src="counterfactual_explanation.png" alt="Counterfactual Explanation" width="700">
<p style="text-align: center; font-size: 14px; color: #555; margin-top: 10px;">
Figure 4: Counterfactual explanation for a rejected loan, comparing current values to desired values to achieve approval.
</p>
</div>
<hr>
</section>
</section>
<section id="applications-of-xai-in-real-world-scenarios" class="level1">
<h1>Applications of XAI in Real-World Scenarios</h1>
<section id="healthcare" class="level3">
<h3 class="anchored" data-anchor-id="healthcare">1. Healthcare</h3>
<p>AI-powered diagnostic tools are transforming medicine, but physicians often hesitate to adopt these technologies without clear explanations. XAI can identify key symptoms driving a diagnosis, empowering clinicians to validate and trust AI recommendations.</p>
<ul>
<li><strong>Case Study</strong>: An XAI-enabled cancer diagnosis tool might highlight specific imaging features contributing to a malignant classification, helping radiologists confirm the results.</li>
</ul>
</section>
<section id="finance" class="level3">
<h3 class="anchored" data-anchor-id="finance">2. Finance</h3>
<p>XAI plays a crucial role in ensuring fairness and transparency in financial decision-making. Loan applicants denied credit can receive detailed explanations, building trust and reducing disputes.</p>
<ul>
<li><strong>Regulatory Compliance</strong>: The GDPR mandates that automated decisions affecting individuals must be explainable, making XAI a necessity for European financial institutions.</li>
</ul>
</section>
<section id="autonomous-vehicles" class="level3">
<h3 class="anchored" data-anchor-id="autonomous-vehicles">3. Autonomous Vehicles</h3>
<p>Self-driving cars rely on AI for navigation, obstacle detection, and decision-making. XAI can clarify why a vehicle chose a particular route or avoided an obstacle, improving public trust and safety.</p>
<hr>
</section>
</section>
<section id="challenges-in-implementing-xai" class="level1">
<h1>Challenges in Implementing XAI</h1>
<p>While XAI holds immense promise, its adoption is not without challenges:</p>
<section id="accuracy-vs.-interpretability" class="level3">
<h3 class="anchored" data-anchor-id="accuracy-vs.-interpretability">1. Accuracy vs.&nbsp;Interpretability</h3>
<p>Complex models like deep neural networks often outperform simpler models but are harder to explain. This trade-off forces practitioners to choose between accuracy and interpretability.</p>
</section>
<section id="scalability" class="level3">
<h3 class="anchored" data-anchor-id="scalability">2. Scalability</h3>
<p>Explaining predictions for large-scale models can be computationally expensive. For instance, generating SHAP values for deep learning models may take hours or require significant computational resources.</p>
</section>
<section id="audience-diversity" class="level3">
<h3 class="anchored" data-anchor-id="audience-diversity">3. Audience Diversity</h3>
<p>Different stakeholders (e.g., data scientists, executives, end-users) require tailored explanations. A SHAP plot might be intuitive for data scientists but incomprehensible to non-technical users.</p>
</section>
<section id="evolving-regulations" class="level3">
<h3 class="anchored" data-anchor-id="evolving-regulations">4. Evolving Regulations</h3>
<p>As governments introduce stricter regulations, organizations face increasing pressure to meet explainability standards. However, the lack of global consistency in these regulations adds complexity.</p>
<hr>
</section>
</section>
<section id="future-of-xai" class="level1">
<h1>Future of XAI</h1>
<p>The future of XAI depends on overcoming its challenges and embracing new opportunities. Here are some potential developments:</p>
<section id="standardization" class="level3">
<h3 class="anchored" data-anchor-id="standardization">1. Standardization</h3>
<p>Establishing industry-wide standards for explainability metrics will enable consistent evaluation and comparison of AI models.</p>
</section>
<section id="integration-with-regulatory-frameworks" class="level3">
<h3 class="anchored" data-anchor-id="integration-with-regulatory-frameworks">2. Integration with Regulatory Frameworks</h3>
<p>Governments are likely to mandate XAI in sensitive domains like healthcare and finance, driving further innovation in this space.</p>
</section>
<section id="advances-in-deep-learning-explainability" class="level3">
<h3 class="anchored" data-anchor-id="advances-in-deep-learning-explainability">3. Advances in Deep Learning Explainability</h3>
<p>Emerging techniques, such as <strong>Integrated Gradients</strong> and <strong>Neuro-Symbolic AI</strong>, aim to make deep learning models more interpretable without sacrificing accuracy.</p>
</section>
<section id="multi-stakeholder-explanations" class="level3">
<h3 class="anchored" data-anchor-id="multi-stakeholder-explanations">4. Multi-Stakeholder Explanations</h3>
<p>Future XAI tools will cater to diverse audiences by offering layered explanations. For instance, a high-level summary for executives and detailed visualizations for data scientists.</p>
<hr>
</section>
</section>
<section id="conclusion" class="level1">
<h1>Conclusion</h1>
<p>Explainable AI is no longer optional—it is a necessity for building trust, ensuring fairness, and achieving regulatory compliance in AI systems. By making AI more interpretable, XAI not only empowers stakeholders but also paves the way for ethical, accountable, and widely adopted AI technologies.</p>
<p>As XAI tools and techniques continue to evolve, their impact will ripple across industries, fostering transparency and trust in the age of intelligent machines.</p>
<p><strong>Thank you for reading!</strong> I’d love to hear your thoughts on XAI or answer any questions you may have.</p>
<hr>
</section>
<section id="references" class="level1">
<h1>References</h1>
<ol type="1">
<li><p>Lundberg, S. M., &amp; Lee, S.-I. (2017). A unified approach to interpreting model predictions. <em>Advances in Neural Information Processing Systems (NeurIPS)</em>. <a href="https://proceedings.neurips.cc/paper/2017/file/8a20a8621978632d76c43dfd28b67767-Paper.pdf">Read the NeurIPS Paper</a></p></li>
<li><p>Ribeiro, M. T., Singh, S., &amp; Guestrin, C. (2016). “Why should I trust you?”: Explaining the predictions of any classifier. <em>Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD ’16)</em>. <a href="https://dl.acm.org/doi/10.1145/2939672.2939778">Access the KDD Paper</a></p></li>
<li><p>Ribeiro, M. T. (n.d.). LIME (Local Interpretable Model-agnostic Explanations). <a href="https://github.com/marcotcr/lime">Visit the LIME GitHub Repository</a></p></li>
<li><p>Lundberg, S. M. (n.d.). SHAP (SHapley Additive Explanations). <a href="https://github.com/slundberg/shap">Visit the SHAP GitHub Repository</a></p></li>
</ol>


</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    // For code content inside modals, clipBoardJS needs to be initialized with a container option
    // TODO: Check when it could be a function (https://github.com/zenorocha/clipboard.js/issues/860)
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->




</body></html>
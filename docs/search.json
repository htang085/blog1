[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "posts/xai-explained/index.html",
    "href": "posts/xai-explained/index.html",
    "title": "The Evolution of Explainable AI (XAI) in Modern Machine Learning",
    "section": "",
    "text": "Artificial Intelligence (AI) has revolutionized industries, enabling advancements in healthcare, finance, transportation, and beyond. However, despite its transformative potential, many AI systems operate as “black boxes”, meaning their internal decision-making processes remain opaque. This lack of transparency raises pressing concerns about fairness, accountability, and trust in AI.\nExplainable AI (XAI) addresses these challenges by making AI systems more interpretable and their decisions more understandable. In this blog, we’ll delve into the evolution of XAI, explore its key concepts and methods, analyze real-world applications, and consider the challenges it faces as it moves toward widespread adoption."
  },
  {
    "objectID": "posts/xai-explained/index.html#key-concepts-in-xai",
    "href": "posts/xai-explained/index.html#key-concepts-in-xai",
    "title": "The Evolution of Explainable AI (XAI) in Modern Machine Learning",
    "section": "Key Concepts in XAI",
    "text": "Key Concepts in XAI\n\nTransparency\nTransparency ensures that the decision-making process of an AI system is accessible and visible. For instance, rule-based systems like decision trees are inherently transparent, whereas neural networks often require post-hoc interpretability techniques to make sense of their complex layers.\n\n\nInterpretability\nInterpretability measures how easily a human can understand a model’s predictions or internal mechanics. For example, a simple linear regression model is more interpretable than a deep learning model because its relationships are mathematically explicit.\n\n\nTrustworthiness\nTrust in AI systems is built when users feel confident in their decisions. Trustworthiness is closely linked to transparency and interpretability. For example, a hospital using AI for cancer diagnosis needs to ensure its system explains why a certain tumor is classified as malignant.\nThese principles form the foundation of XAI, bridging the gap between sophisticated algorithms and human understanding."
  },
  {
    "objectID": "posts/xai-explained/index.html#shap-shapley-additive-explanations",
    "href": "posts/xai-explained/index.html#shap-shapley-additive-explanations",
    "title": "The Evolution of Explainable AI (XAI) in Modern Machine Learning",
    "section": "1. SHAP (SHapley Additive Explanations)",
    "text": "1. SHAP (SHapley Additive Explanations)\nSHAP uses concepts from cooperative game theory to assign importance values to features based on their contribution to the model’s predictions. SHAP provides both global explanations (understanding the overall model behavior) and local explanations (analyzing individual predictions).\n\nExample: For a loan approval model, a SHAP plot might reveal that “income” positively contributes to approval, while “credit history” negatively affects it.\n\n\n\n\nFigure 1: SHAP summary plot showing feature importance and their contributions to model predictions."
  },
  {
    "objectID": "posts/xai-explained/index.html#lime-local-interpretable-model-agnostic-explanations",
    "href": "posts/xai-explained/index.html#lime-local-interpretable-model-agnostic-explanations",
    "title": "The Evolution of Explainable AI (XAI) in Modern Machine Learning",
    "section": "2. LIME (Local Interpretable Model-Agnostic Explanations)",
    "text": "2. LIME (Local Interpretable Model-Agnostic Explanations)\nLIME approximates a model locally by building an interpretable surrogate model (e.g., linear regression) around a specific prediction. This method is particularly useful for black-box models like neural networks.\n\nExample: LIME explains why an AI system classifies an email as spam by showing the words most likely to trigger the classification.\n\n\n\n\nFigure 2: LIME explanation plot highlighting feature contributions to an individual prediction."
  },
  {
    "objectID": "posts/xai-explained/index.html#feature-importance-visualizations",
    "href": "posts/xai-explained/index.html#feature-importance-visualizations",
    "title": "The Evolution of Explainable AI (XAI) in Modern Machine Learning",
    "section": "3. Feature Importance Visualizations",
    "text": "3. Feature Importance Visualizations\nMany machine learning libraries, such as scikit-learn and XGBoost, include built-in tools to visualize feature importance. These tools help identify the most impactful features in a dataset. In the case of Random Forest, feature importance is derived from the average decrease in impurity across all the decision trees in the forest. This makes it a reliable measure for understanding how different features contribute to the model’s predictions.\n\n\n\nFigure 3: Feature importance plot from a Random Forest Classifier. Each bar represents the average decrease in impurity associated with the corresponding feature."
  },
  {
    "objectID": "posts/xai-explained/index.html#counterfactual-explanations",
    "href": "posts/xai-explained/index.html#counterfactual-explanations",
    "title": "The Evolution of Explainable AI (XAI) in Modern Machine Learning",
    "section": "4. Counterfactual Explanations",
    "text": "4. Counterfactual Explanations\nCounterfactuals answer “what if” questions, such as: “What changes would result in a different outcome?” These explanations are actionable and help users understand how to achieve a desired result.\n\nExample: A counterfactual explanation for a rejected loan might suggest increasing income by $5,000 or reducing outstanding debt by 20%.\n\n\n\n\nFigure 4: Counterfactual explanation for a rejected loan, comparing current values to desired values to achieve approval."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Blog",
    "section": "",
    "text": "Welcome to my blog! Here, I share insights, tutorials, and my journey in data science.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe Evolution of Explainable AI (XAI) in Modern Machine Learning\n\n\n\n\n\n\nMachine Learning\n\n\nData Science\n\n\nExplainable AI\n\n\n\n\n\n\n\n\n\nJan 18, 2025\n\n\nHui Tang\n\n\n\n\n\n\nNo matching items"
  }
]